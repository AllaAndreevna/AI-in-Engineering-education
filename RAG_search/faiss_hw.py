# -*- coding: utf-8 -*-
"""FAISS HW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G3IiGPcGHXPtiw1wBScw3kqiMCSbIaP5
"""

# -*- coding: utf-8 -*-
"""FAISS_LABA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_sfo4_1pmvixEDyqkSGBXtehfqUKXwd
"""

!pip install --upgrade pip

!pip install faiss-cpu sentence-transformers langchain langchain-groq pandas tqdm transliterate

!pip install -U langchain-community
!pip install -U langchain

import os
import faiss
import numpy as np
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_groq import ChatGroq
from transliterate import translit
from tqdm import tqdm

os.environ["GROQ_API_KEY"] = "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"

CSV_FILES = {
    "test": "/content/LR2_dev.csv",
    "answers": "/content/LR2_dev_answer.csv",
    "full": "/content/LR2.csv"
}
BOOKS_FOLDER = "/content/drive/MyDrive/dataset/"
OUTPUT_CSV = "/content/LR2_dev_predictions.csv"

print("‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, API –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")

# üìú –ó–∞–≥—Ä—É–∂–∞–µ–º CSV, —É–±–∏—Ä–∞—è –Ω–µ–Ω—É–∂–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
df = pd.read_csv(CSV_FILES["full"]).drop(columns=["Unnamed: 0"], errors="ignore")

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–æ–º–µ—Ä–∞–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (–Ω–∞—á–∏–Ω–∞—è —Å 1)
df.insert(0, "id", df.index)

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
df.columns = ["id", "question", "answer_a", "answer_b", "answer_c", "answer_d", "book"]

# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
print(f"üìå –ö–æ–ª–æ–Ω–∫–∏ –≤ —Ñ–∞–π–ª–µ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {df.columns.tolist()}")
print(df.head())

from google.colab import drive
import os

# –ü–æ–¥–∫–ª—é—á–∞–µ–º Google Drive
drive.mount('/content/drive')

import os

# üìÇ –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∫–Ω–∏–≥–∞–º–∏
BOOKS_FOLDER = "/content/drive/MyDrive/dataset/"

# üìñ –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö {–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏: —Ç–µ–∫—Å—Ç}
books_data = {}

# üîÑ –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –≤—Å–µ—Ö –ø–∞–ø–æ–∫ –∏ —Ñ–∞–π–ª–æ–≤
for root, dirs, files in os.walk(BOOKS_FOLDER):
    for file in files:
        if file.endswith(".txt"):  # –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
            book_path = os.path.join(root, file)
            try:
                with open(book_path, "r", encoding="utf-8") as f:
                    books_data[file.replace(".txt", "")] = f.read()
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {book_path}: {e}")

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(books_data)} –∫–Ω–∏–≥.")


book_name_mapping = {
    "–†–æ–∫–æ–≤—ã–µ —è–π—Ü–∞": "Bulgakov_RokovyeYayca",
    "–¢–∞—Ä–∞—Å –ë—É–ª—å–±–∞": "Gogol_TarasBulba",
    "–ú–µ—Ä—Ç–≤—ã–µ –¥—É—à–∏": "Gogol_MertvieDushi",
    "–î–µ–ª–æ –ê—Ä—Ç–∞–º–æ–Ω–æ–≤—ã—Ö": "Gorkiyi_DeloArtamonovyih",
    "–ì–æ—Ä–æ–¥ –±–µ–∑ –ø–∞–º—è—Ç–∏": "Bulichev_Gorod_bez_pamyati",
    "–ö–Ω—è–≥–∏–Ω—è –õ–∏–≥–æ–≤—Å–∫–∞—è": "Lermontov_KnyaginyaLigovskaya",
    "–ü–æ–ª–æ—Ç–µ–Ω—Ü–µ —Å –ü–µ—Ç—É—Ö–æ–º": "Bulgakov_PolotenceSPetuhom",
    "–ó–≤–µ–∑–¥–Ω–∞—è —Å—ã–ø—å": "Bulgakov_ZvezdnayaSyp",
    "–°—Ç–∞–ª—å–Ω–æ–µ –≥–æ—Ä–ª–æ": "Bulgakov_StalnoeGorlo",
    "–í–∏–π": "Gogol_Vii",
    "–ö–∞–∫ –ø–æ—Å—Å–æ—Ä–∏–ª—Å—è –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á —Å –ò–≤–∞–Ω–æ–º –ù–∏–∫–∏—Ñ–æ—Ä–æ–≤–∏—á–µ–º": "Gogol_KakPossorilsyaIvanIvanovichSIvanomNikiforovichem",
    "–ú–∞—Å—Ç–µ—Ä –∏ –ú–∞—Ä–≥–∞—Ä–∏—Ç–∞": "Bulgakov_MasterIMargarita",
    "–ê—Å—è": "Turgenev_Asya",
    "–í–æ–π–Ω–∞ –∏ –º–∏—Ä": "Tolstoy_VoynaIMir1",
    "–ö—Ä–µ—â–µ–Ω–∏–∏ –ø–æ–≤–æ—Ä–æ—Ç–æ–º": "Bulgakov_KrescheniePovorotom",
    "–°—Ç–æ –ª–µ—Ç —Ç–æ–º—É –≤–ø–µ—Ä—ë–¥": "Bulichev_Sto_let_tomu_vpered",
    "–ú—É–º—É": "Turgenev_Mumu",
    "–õ–∏–ª–æ–≤—ã–π —à–∞—Ä": "Bulichev_Lilovii_shar",
    "–ú–æ—Ä—Ñ–∏–π": "Bulgakov_Morfiyi",
    "–ó–∞–ø–∏—Å–∫–∏ –æ—Ö–æ—Ç–Ω–∏–∫–∞": "Turgenev_ZapiskiOhotnika",
    "–¢—å–º–∞ –µ–≥–∏–ø–µ—Ç—Å–∫–∞—è": "Bulgakov_TmaEgipetskaya",
    "–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –ø–æ–∫–æ–π–Ω–∏–∫–∞": "Bulgakov_PriklyucheniyaPokoynika",
    "–°—Ç–∞—Ä–æ—Å–≤–µ—Ç—Å–∫–∏—Ö –ø–æ–º–µ—â–∏–∫–∞—Ö": "Gogol_StarosvetskiyePomeshchiki",
    "–î–µ–∫–∞–±—Ä–∏—Å—Ç—ã": "Tolstoy_Decabristy",
    "–ß–µ–ª–æ–≤–µ–∫-–∞–º—Ñ–∏–±–∏—è": "Belyaev_CheloverAmfibia",
    "–ß–µ—Ä—Ç–æ–≤–∞ –º–µ–ª—å–Ω–∏—Ü–∞": "Belyaev_ChyortovaMelnica",
    "–ì–æ–ª–æ–≤–∞ –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –î–æ—É—ç–ª—è": "Belyaev_GolovaProfessoraDouyelya",
    "–ù–∞–¥ –±–µ–∑–¥–Ω–æ–π": "Belyaev_NadBezdnoyi",
    "–ù–µ–≤–∏–¥–∏–º—ã–π —Å–≤–µ—Ç": "Belyaev_NevidimyiyiSvet",
    "–ù–∏ –∂–∏–∑–Ω—å, –Ω–∏ —Å–º–µ—Ä—Ç—å": "Belyaev_NiJiznNiSmert",
    "–ü—Ä–æ–¥–∞–≤–µ—Ü –≤–æ–∑–¥—É—Ö–∞": "Belyaev_ProdavecVozduhaI",
    "–ü—Ä–æ–ø–∞–≤—à–∏–π –æ—Å—Ç—Ä–æ–≤": "Belyaev_PropavshiyiOstrov",
    "–°–∏–ª—å–Ω–µ–µ –±–æ–≥–∞": "Belyaev_SilneeBoga",
    "–í–µ—Ä—Ö–æ–º –Ω–∞ –≤–µ—Ç—Ä–µ": "Belyaev_VerhomNaVetre",
    "–í–ª–∞—Å—Ç–µ–ª–∏–Ω –º–∏—Ä–∞": "Belyaev_VlastelinMira",
    "–ó–æ–ª–æ—Ç–∞—è –≥–æ—Ä–∞": "Belyaev_ZolotayaGora",
    "–ë–µ—Å—ã": "Dostoevskiyi_BesyiII",
    "–ë—Ä–∞—Ç—å—è –ö–∞—Ä–∞–º–∞–∑–æ–≤—ã": "Dostoevskiyi_BratyaKaramazovyi",
    "–ò–¥–∏–æ—Ç": "Dostoevskiyi_Idiot",
    "–ü—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –∏ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ": "Dostoevskiyi_PrestuplenieINakazanie",
    "–°–±–æ—Ä–Ω–∏–∫": "Gogol_Sbornik",
    "–î–µ—Ç—Å—Ç–≤–æ": "Gorkiyi_Detstvo",
    "–ñ–∏–∑–Ω—å –ö–ª–∏–º–∞ –°–∞–º–≥–∏–Ω–∞": "Gorkiyi_JiznKlimaSamgina",
    "–°—Ç–∞—Ä—É—Ö–∞ –ò–∑–µ—Ä–≥–∏–ª—å": "Gorkiyi_StaruhaIzergil",
    "–í –ª—é–¥—è—Ö": "Gorkiyi_VLyudyah",
    "–î–µ—Ç—Å—Ç–≤–æ. –í –ª—é–¥—è—Ö. –ú–æ–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã": "Gorkiyi_YeksklyuzivrusDetstvoVLyudyahMoiUni",
    "–ì—Ä–∞–Ω–∞—Ç–æ–≤—ã–π –±—Ä–∞—Å–ª–µ—Ç": "Kuprin_GranatovyiyiBraslet",
    "–î–æ–±—Ä—ã–π –¥–æ–∫—Ç–æ—Ä": "Kuprin_HrestomatiidlyDobryiyiDoktor",
    "–û–ª–µ—Å—è": "Kuprin_Olesya",
    "–ü–æ–µ–¥–∏–Ω–æ–∫": "Kuprin_Poedinok",
    "–°—É–ª–∞–º–∏—Ñ—å": "Kuprin_Sulamif",
    "–Ø–º–∞": "Kuprin_Yama",
    "–ê—à–∏–∫-–ö–µ—Ä–∏–±": "Lermontov_AshikKerib",
    "–ì–µ—Ä–æ–π –Ω–∞—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏": "Lermontov_GeroyiNashegoVremeni",
    "–•—Ä–µ—Å—Ç–æ–º–∞—Ç–∏—è –¥–ª—è –ê—à–∏–∫-–ö–µ—Ä–∏–±": "Lermontov_HrestomatiidlyAshikKerib",
    "–ö–Ω–∏—è–≥–∏–Ω—è –õ–∏–≥–æ–≤—Å–∫–∞—è": "Lermontov_KnyaginyaLigovskaya",
    "–®—Ç–æ—Å—Å": "Lermontov_Shtoss",
    "–ë–æ–≥–∞—á –∏ –ï—Ä–µ–º–∫–∞": "MaminSibiryak_BogachIEremka",
    "–ï–º–µ–ª—è-–æ—Ö–æ—Ç–Ω–∏–∫": "MaminSibiryak_EmelyaOhotnik",
    "–ì–æ—Ä–Ω–æ–µ –≥–Ω–µ–∑–¥–æ": "MaminSibiryak_GornoeGnezdo",
    "–ü–æ—Å—Ç–æ–π–∫–æ": "MaminSibiryak_Postoyiko",
    "–ü—Ä–∏—ë–º—ã—à": "MaminSibiryak_Priemyish",
    "–ü—Ä–∏–≤–∞–ª–æ–≤—Å–∫–∏–µ –º–∏–ª–ª–∏–æ–Ω—ã": "MaminSibiryak_PrivalovskieMillionyi",
    "–°–µ—Ä–∞—è —à–µ–π–∫–∞": "MaminSibiryak_SerayaSheyika",
    "–í–µ—Ä—Ç–µ–ª": "MaminSibiryak_Vertel",
    "–í –≥–ª—É—à–∏": "MaminSibiryak_VGlushiI",
    "–ó–æ–ª–æ—Ç–æ": "MaminSibiryak_Zoloto",
    "–ê–ø–µ–ª–ª–µ—Å–æ–≤–∞ —á–µ—Ä—Ç–∞": "Pasternak_ApellesovaCherta",
    "–î–æ–∫—Ç–æ—Ä –ñ–∏–≤–∞–≥–æ": "Pasternak_DoktorZhivago",
    "–°—É–¥—å–±–∞ —á–µ–ª–æ–≤–µ–∫–∞": "Sholohov_SudbaCheloveka",
    "–ü–æ–¥–Ω—è—Ç–∞—è —Ü–µ–ª–∏–Ω–∞": "Sholokhov_PodnyatayaTselina",
    "–¢–∏—Ö–∏–π –î–æ–Ω": "Sholokhov_TikhiyDon",
    "–î–µ—Ç—Å—Ç–≤–æ": "Tolstoy_Detstvo",
    "–û—Ç—Ä–æ—á–µ—Å—Ç–≤–æ": "Tolstoy_Otrochestvo",
    "–°–µ–º–µ–π–Ω–æ–µ —Å—á–∞—Å—Ç—å–µ": "Tolstoy_SemeynoyeSchastye",
    "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": "Tolstoy_Voskreseniye",
    "–í–æ–π–Ω–∞ –∏ –ú–∏—Ä 1": "Tolstoy_VoynaIMir1",
    "–í–æ–π–Ω–∞ –∏ –ú–∏—Ä 2": "Tolstoy_VoynaIMir2",
    "–Æ–Ω–æ—Å—Ç—å": "Tolstoy_Yunost",
    "–ê—Å—è": "Turgenev_Asya",
    "–î–≤–æ—Ä—è–Ω—Å–∫–æ–µ –≥–Ω–µ–∑–¥–æ": "Turgenev_DvoryanskoeGnezdo",
    "–ú—É–º—É": "Turgenev_Mumu",
    "–ù–∞–∫–∞–Ω—É–Ω–µ": "Turgenev_Nakanune",
    "–û—Ç—Ü—ã –∏ –¥–µ—Ç–∏": "Turgenev_OtcyiIDeti",
    "–ü–µ—Ä–≤–∞—è –ª—é–±–æ–≤—å": "Turgenev_PervayaLyubov",
    "–ó–∞–ø–∏—Å–∫–∏ –æ—Ö–æ—Ç–Ω–∏–∫–∞": "Turgenev_ZapiskiOhotnika"

}

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from tqdm import tqdm

# üîß –†–∞–∑–±–∏–≤–∞–µ–º –∫–Ω–∏–≥–∏ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

# üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={"device": "cuda"}  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU
)

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è FAISS —Ö—Ä–∞–Ω–∏–ª–∏—â
vector_stores = {}

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â
for book_name, text in tqdm(books_data.items(), desc="–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â", unit="–∫–Ω–∏–≥–∞"):
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    split_documents = splitter.create_documents([text])

    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
    vector_store = FAISS.from_documents(split_documents, embedding_model)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vector_stores[book_name] = vector_store

print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è {len(vector_stores)} –∫–Ω–∏–≥.")

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_groq import ChatGroq
import random

# ‚ö° –ü–æ–¥–∫–ª—é—á–∞–µ–º LLM
llm = ChatGroq(model_name="llama3-70b-8192")

# üí¨ –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM
prompt = PromptTemplate.from_template("""
–ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫–Ω–∏–≥–∏, –≤—ã–±–µ—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å.
–í—ã–±–µ—Ä–∏ **–¢–û–õ–¨–ö–û –û–î–ù–£ –ë–£–ö–í–£** (A, B, C –∏–ª–∏ D), –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å:
{question}

A) {option_a}
B) {option_b}
C) {option_c}
D) {option_d}

–û—Ç–≤–µ—Ç:  """)

# üîó –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É –¥–ª—è LLM
chain = LLMChain(prompt=prompt, llm=llm)

# üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º A, B, C, D ‚Üí 1, 2, 3, 4
answer_map = {"A": 1, "B": 2, "C": 3, "D": 4}

# üèÜ –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
def find_correct_answer(question_index):
    row = df.iloc[question_index]  # –ë–µ—Ä–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ –∏–Ω–¥–µ–∫—Å—É

    original_book_name = row["book"].strip()  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ CSV
    book_name = book_name_mapping.get(original_book_name, None)  # –ò—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ

    if not book_name:
        print(f"‚ö†Ô∏è –ö–Ω–∏–≥–∞ '{original_book_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        return -1  # –ï—Å–ª–∏ –∫–Ω–∏–≥–∏ –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º -1

    query = row["question"]

    # üîé –ò—â–µ–º –æ—Ç–≤–µ—Ç –≤ –∫–Ω–∏–≥–µ
    if book_name in vector_stores:
        context_docs = vector_stores[book_name].similarity_search(query, k=3)
        context = "\n\n".join([doc.page_content for doc in context_docs])
    else:
        context = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω."

    # print(f"üìñ –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {question_index}:\n{context}")

    # üß† –ó–∞–ø—É—Å–∫–∞–µ–º LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞
    result = chain.invoke({
        "context": context,
        "question": query,
        "option_a": row["answer_a"],
        "option_b": row["answer_b"],
        "option_c": row["answer_c"],
        "option_d": row["answer_d"]
    })
    # print(f"üîç –û—Ç–≤–µ—Ç –æ—Ç LLM: {result}")

    if isinstance(result, dict):
        result = result.get("text", "").strip().upper()

    # üéØ –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –æ—Ç–≤–µ—Ç–∞
    result = result[:1] if result and result[0] in "ABCD" else "A"

    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç: {result}")
    return answer_map.get(result, 1)  # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º -1


# üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
results = []
for question_index in range(len(df)):  # –ò–¥–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
    correct_answer = find_correct_answer(question_index)
    results.append({"id": question_index, "answer": correct_answer})
    print(f"‚úÖ –í–æ–ø—Ä–æ—Å {question_index}: –û—Ç–≤–µ—Ç {correct_answer}")

# üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV
output_csv = "/content/LR2_predictions.csv"
results_df = pd.DataFrame(results)
results_df.to_csv(output_csv, index=False)

print(f"üéØ –§–∞–π–ª —Å –æ—Ç–≤–µ—Ç–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_csv}")

# –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–Ω–∏–≥–∏ —É –Ω–∞—Å –µ—Å—Ç—å
print("üìö –ö–Ω–∏–≥–∏ –≤ vector_stores:", list(vector_stores.keys()))
print("üìö –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ CSV:", df["book"].unique())